<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Flow-Guided Feature Aggregation for Video Object Detection å­¦ä¹ ç¬”è®°</title>
<link rel="shortcut icon" href="../../assets/img/icon.png" type="image/x-icon"/>
<link rel="icon" href="../../assets/img/icon.png" type="image/x-icon"/>
<link rel="stylesheet" href="../../assets/css/github-markdown.css">
<link rel="stylesheet" href="../../assets/css/index.css">
</head>
<body class="markdown-body">
<p><a href="https://painterdrown.github.io">painterdrown Blog</a> - <a href="https://painterdrown.github.io/cv">painterdrown CV</a></p>
<h1>Flow-Guided Feature Aggregation for Video Object Detection å­¦ä¹ ç¬”è®°</h1>
<blockquote>
<p>â° 2018-06-02 09:40:36<br/>
ğŸ‘¨ğŸ»â€ğŸ’» painterdrown</p>
</blockquote>
<p>[TOC]</p>
<h2>0. Abstract</h2>
<p>ç›®å‰çš„è§†é¢‘ç›®æ ‡æ£€æµ‹ç½‘ç»œéƒ½ä¸æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå« flow-guided feature aggregationï¼ˆæµå¯¼å‘ç‰¹å¾èšé›†ï¼‰ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŠ ä¸‹æ¥æˆ‘ç®€ç§°ä¸º <strong>FGFA</strong>ã€‚</p>
<blockquote>
<p>It leverages temporal coherence on feature level instead.</p>
</blockquote>
<p>è¿™å¥è¯çš„æ„æ€æ˜¯è¯´ï¼ŒFFA å…³æ³¨å¦‚ä½•åˆ©ç”¨ feature level çš„æ—¶é—´è¿è´¯æ€§ä¿¡æ¯ï¼Œå¹¶ä¸”åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥è¾¾åˆ°å¥½çš„æ£€æµ‹æ•ˆæœã€‚</p>
<p>FFA èšé›†äº†åŒä¸€è¿åŠ¨è·¯å¾„ (motion path) ä¸Šçš„ç‰¹å¾ä¿¡æ¯ã€‚</p>
<h2>1. Introduction</h2>
<p>ç›®å‰æœ€å¸…çš„å›¾åƒç›®æ ‡æ£€æµ‹æ¡†æ¶åŸºæœ¬éƒ½æ˜¯ <strong>Deep Convolutional Neural Networks</strong>ï¼Œä½†æ˜¯å®ƒåœ¨è§†é¢‘å¸§ä¸­è¡¨ç°æ¬ ä½³â€”â€”åŸå› æ˜¯è§†é¢‘å¸§ä¸­ <em>motion blur</em>ï¼ˆå›¾åƒè¿åŠ¨çš„åŒºåŸŸå®¹æ˜“æ¨¡ç³Šï¼‰ çš„ç°è±¡æ¯”è¾ƒæ˜æ˜¾ï¼Œè¿˜æœ‰ <em>video defocus</em>ï¼Œ<em>rare poses</em> ç­‰åŸå› ã€‚</p>
<blockquote>
<p>The performance improvement is from heuristic post-processing instead of principled learning.</p>
</blockquote>
<p>ç›®å‰çš„è§†é¢‘æ£€æµ‹æ¡†æ¶ä¹Ÿæ˜¯ <em>box level</em> çš„ï¼šåœ¨æ£€æµ‹å‡ºå…³é”®å¸§ä¹‹åï¼Œåç»­çš„ bounding box æ£€æµ‹æ˜¯æ¯”è¾ƒä¼ ç»Ÿç²—æš´çš„ï¼ˆåŸºäº motion estimation æˆ–è€… optical flowï¼‰ã€‚è¿™ç§åšæ³•æ•ˆæœå¾€å¾€å¾ˆå¹³åº¸ã€‚</p>
<p>ä½†æ˜¯ï¼Œä¸€äº›æ¯”è¾ƒå·®çš„ feature aggregation åšæ³•ä¼šå— <em>video motion</em> çš„å½±å“ï¼ˆåŒä¸€ä¸ªç›®æ ‡åœ¨ç›¸é‚»å¸§ä¹‹é—´ç©ºé—´ä¸å¯¹é½ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥ç ”ç©¶å¦‚ä½•åœ¨æ·±åº¦å­¦ä¹ ä¸­æ¨¡å‹åŒ–è¿™äº› motionã€‚</p>
<p><img src="images/architecture.png" alt=""></p>
<p>FGFA æ¶‰åŠäº†å››ä¸ªç½‘ç»œï¼š</p>
<ul>
<li>
<p><strong>feature extraction network</strong>ã€‚ç”¨æ¥æå– reference frameï¼ˆå¯ä»¥ç†è§£ä¸ºå½“å‰å¸§ï¼‰çš„ç‰¹å¾ã€‚</p>
</li>
<li>
<p><strong>optical flow network</strong>ã€‚ç”¨æ¥ä¼°è®¡é‚»è¿‘å¸§ä¹‹é—´çš„è¿åŠ¨ä¿¡æ¯ã€‚ç„¶ååŸºäº reference frameï¼Œæ ¹æ®è¿™ä¸ªè¿åŠ¨ä¿¡æ¯ï¼Œå¯¹é‚»è¿‘å¸§åš warpingï¼ˆå˜å½¢ï¼‰ã€‚</p>
</li>
<li>
<p><strong>adaptive weighting network</strong>ã€‚ç”¨æ¥åœ¨ reference frame çš„ feature maps ä¸Šé¢èšé›†å˜å½¢åçš„é‚»è¿‘å¸§çš„ feature mapsã€‚</p>
</li>
<li>
<p><strong>detection network</strong>ã€‚èšé›†åçš„ feature maps ä¼šè¾“å…¥åˆ°è¯¥ç½‘ç»œï¼Œæ¥æ£€æµ‹ reference frame ä¸Šçš„ç›®æ ‡ã€‚</p>
</li>
</ul>
<p>å¦å¤–ï¼ŒFGFA æ˜¯ feature level çš„ï¼Œè‹¥æ˜¯ä¸ä¸€äº› box level çš„æ–¹æ³•ç»“åˆäº’è¡¥ï¼Œå¯ä»¥æå‡æ•ˆæœã€‚</p>
<h2>2. Related Work</h2>
<ul>
<li>
<p><strong>Object detection from image</strong>ã€‚è¿™é‡Œæåˆ°äº† R-FCNï¼Œå…¶ä»–ä¸ä½œèµ˜è¿°ã€‚</p>
</li>
<li>
<p><strong>Object detection in video</strong>ã€‚ImageNet æœ‰ä¸€ä¸ªæ–°çš„æ¯”èµ›å« VIDï¼Œç›®å‰å¾ˆå¤šæ–¹æ³•éƒ½æ˜¯ <em>bounding-box post-processing</em> ä¸” <em>multi-stage pipeline</em>ï¼ˆåé¢çš„ stage å¿…é¡»ä¾èµ–äºå‰é¢ stage çš„ç»“æœï¼Œè€Œä¸”ä¸å¥½åšé”™è¯¯æ ¡æ­£ï¼‰ã€‚è¿™æ­£æ˜¯ box level çš„å¼Šç«¯ï¼Œå› æ­¤ï¼ŒFGFA æ˜¯åŸºäº feature level çš„ç«¯åˆ°ç«¯ç½‘ç»œã€‚</p>
</li>
<li>
<p><strong>Motion estimation by flow</strong>ã€‚è¿™é‡Œè®²çš„ä¸œè¥¿å¾ˆå¤šåœ¨ <a href="https://painterdrown.github.io/cv/dff">Deep Feature Flow for Video Recognition å­¦ä¹ ç¬”è®°</a> å·²ç»æåˆ°äº†ï¼Œä¸ä½œèµ˜è¿°ã€‚</p>
</li>
<li>
<p><strong>Feature aggregation</strong>ã€‚å®ƒåœ¨åŠ¨ä½œè¯†åˆ« (action recognition) ä»¥åŠè§†é¢‘æè¿° (video description) ä¸­å·²ç»è¢«å¹¿æ³›åº”ç”¨äº†ï¼Œå¤§å¤šæ•°éƒ½æ˜¯ç”¨ <strong>RNN</strong> æ¥èšé›†é‚»è¿‘å¸§çš„ featureã€‚æ­¤å¤–ï¼Œä¹Ÿæœ‰ä¸€äº›æ˜¯ç”¨å·ç§¯æ¥æå–æ¯”è¾ƒå…¨é¢çš„æ—¶ç©ºç‰¹å¾ (spatial-temporal features)ï¼Œä½†æ˜¯è¿™äº›å·ç§¯æ ¸ä¼šé˜»ç¢é«˜é€Ÿç§»åŠ¨ç›®æ ‡çš„ modelingã€‚ä½†æ˜¯å¦‚ä¸ºäº†æ‰“ç ´è¿™ä¸ªé™åˆ¶è€Œå•å•å¢å¤§å·ç§¯æ ¸çš„å¤§å°çš„è¯ï¼Œåˆ™ä¼šå¸¦æ¥æ¯”è¾ƒå¤šçš„è®¡ç®—å¼€é”€ã€å†…å­˜é—®é¢˜ä»¥åŠè¿‡æ‹Ÿåˆç­‰é—®é¢˜ã€‚å› æ­¤ï¼ŒFGFA ä¾é  flow-guided aggregationï¼ˆå…·æœ‰å¯ä¼¸ç¼©æ€§ï¼‰æ¥å¾—åˆ°ä¸åŒç±»å‹çš„ç›®æ ‡è¿åŠ¨ä¿¡æ¯ã€‚</p>
</li>
<li>
<p><strong>Visual tracking</strong>ã€‚ç°åœ¨åŸºæœ¬éƒ½ç”¨æ·±åº¦ CNN æ¥åšç›®æ ‡è¿½è¸ªã€‚è€Œç›®æ ‡è¿½è¸ªä¸ç›®æ ‡æ£€æµ‹åˆæœ‰åŒºåˆ«ï¼šå‰è€…ä¼šå…ˆå‡è®¾ç›®æ ‡çš„åˆå§‹ä½ç½®ï¼Œä¸”ä¸è¦æ±‚åšåˆ†ç±»ã€‚</p>
</li>
</ul>
<h2>3. Flow Guided Feature Aggregation</h2>
<h3>3.1. Model Design</h3>
<p>é¦–å…ˆï¼Œç”¨æ·±å±‚å·ç§¯è®¡ç®—å‡º reference frame I~i~ çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡ <a href="../papers/FlowNet.pdf">FlowNet</a> <strong>N~flow~</strong> æ¥æ¨å¯¼å‡ºå…¶ neighbor frame I~j~ çš„ç‰¹å¾ï¼Œç´§æ¥ç€è¿™ä¸ªç‰¹å¾å†åšä¸€ä¸ª warpingã€‚</p>
<p>å¾—åˆ°ä¸€ç³»åˆ—çš„ warping åçš„ç‰¹å¾ï¼Œå°±æ‹¿æ¥åš <strong>feature aggregation</strong>ã€‚èšåˆåçš„ç‰¹å¾åŒ…å«äº†å¦‚ illuminations/viewpoints/poses/non-rigid ç­‰ä¿¡æ¯ã€‚</p>
<p><img src="images/aggregation.png" alt=""></p>
<p>ç‰¹å¾èšé›†æ˜¯é€šè¿‡åŠ æƒç›¸åŠ å¾—åˆ°çš„ï¼Œç¦» reference frame è¶Šè¿‘çš„å¸§ï¼Œæƒé‡è¶Šå¤§ã€‚ç”¨ <strong>cosine similarity metric</strong> æ¥è¡¡é‡å˜æ€§åç‰¹å¾ä¸ reference frame çš„ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚è¦æ³¨æ„çš„æ˜¯ï¼Œè®¡ç®—ç›¸ä¼¼åº¦ä¸æ˜¯ç›´æ¥ç”¨çš„ featureï¼Œè€Œæ˜¯æŠŠ feature å†ç»è¿‡ä¸€ä¸ª <strong>tiny fully convolutional network</strong>ï¼Œç›®çš„æ˜¯å°†ç‰¹å¾æŠ•å½±æˆä¸€ä¸ª new embeddingï¼ˆæˆ‘ä¹Ÿä¸æ‡‚æ˜¯ä»€ä¹ˆï¼‰ï¼Œè¿™æ ·èƒ½æ›´æ–¹ä¾¿åé¢çš„ç½‘ç»œå»åšç›¸ä¼¼æ€§è®¡ç®—ã€‚</p>
<h3>3.2. Training and Inference</h3>
<p>Inference çš„ä¼ªä»£ç å¦‚ä¸‹ï¼Œå¯ä»¥æè¿°ä¸ºï¼š</p>
<ol>
<li>é¦–å…ˆç”¨ N~feat~ å¯¹è§†é¢‘çš„æ¯ä¸€å¸§éƒ½ç®—å‡ºå…¶å·ç§¯ç‰¹å¾å›¾</li>
<li>ä¾æ¬¡å°†æ¯ä¸€å¸§ä½œä¸º reference frameï¼Œé€šè¿‡ä¸Šè¿°çš„æ–¹æ³•ç®—å‡ºå…¶èšé›†åçš„ç‰¹å¾</li>
<li>å°†èšé›†åçš„ç‰¹å¾æ”¾è¿› N~det~ è¿›è¡Œç›®æ ‡æ£€æµ‹</li>
<li>æ›´æ–° <strong>feature buffer</strong>ï¼Œè¿™é‡Œå¯¹åº”ä¼ªä»£ç çš„ç¬¬ 13 è¡Œã€‚æˆ‘æ€è€ƒäº†ä¸€ä¸‹ç»ˆäºçŸ¥é“è¿™ä¸€æ­¥çš„æ„ä¹‰ï¼šç®—æ³•ä¸€å¼€å§‹çš„æ—¶å€™ï¼Œä¸æ˜¯ç›´æ¥æŠŠæ‰€æœ‰å¸§çš„ç‰¹å¾éƒ½ç®—å‡ºæ¥äº†ï¼Œå› ä¸ºé‚£æ ·å­å¤ªå å†…å­˜ã€‚å› æ­¤åªè™šå…ˆç®—å‰ K ä¸ªç‰¹å¾ï¼Œç»´æŠ¤ä¸€ä¸ªé•¿åº¦ä¸º K çš„ feature bufferã€‚åœ¨æ¯ä¸€è½®è¿­ä»£ä¹‹åï¼Œéƒ½ä¼šè®¡ç®—ä¸‹ä¸€ä¸ª feature å¡è¿› buffer é‡Œé¢ã€‚</li>
</ol>
<p><img src="images/code.png" alt=""></p>
<p>æ•´ä¸ª FGFA æ¶æ„æ˜¯å¯å¯¼è€Œä¸”ç«¯åˆ°ç«¯çš„ã€‚è®­ç»ƒçš„æ—¶å€™ï¼Œç”±äºå†…å­˜çš„é™åˆ¶ï¼ŒK åªèƒ½å–ä¸€ä¸ªæ¯”è¾ƒå°çš„å€¼ (K = 2)ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ª <strong>temporal dropout</strong> çš„è¯´æ³•ã€‚ä¸æ˜¯è¯´è®­ç»ƒçš„æ—¶å€™åªåœ¨å‰åå„ä¸¤ä¸ªé‚»è¿‘å¸§ä¹‹é—´é‡‡æ ·ï¼Œé‡‡æ ·çš„èŒƒå›´æ˜¯è·Ÿå‰é¢çš„ inference çš„èŒƒå›´ä¸€æ ·ï¼Œåªæ˜¯è®­ç»ƒçš„æ—¶å€™åªå‰åå„é‡‡æ · 2 å¸§ï¼Œæ‰€ä»¥è¿™é‡Œè¦ç†è§£å¥½ K = 2 çš„å«ä¹‰ã€‚</p>
<h3>3.3. Network Architecture</h3>
<ul>
<li><strong>Flow network</strong>: FlowNet (â€œsimpleâ€ version)</li>
<li><strong>Feature network</strong>: ResNet (-50 and -101) and Inception-Resnet</li>
<li><strong>Embedding network</strong>: 3 layers (randomly initialized):
<ul>
<li>1Ã—1Ã—512 convolution</li>
<li>3Ã—3Ã—512 convolution</li>
<li>1Ã—1Ã—2048 convolution</li>
</ul>
</li>
<li><strong>Detection network</strong>: R-FCN</li>
</ul>
<h2>4. Experiments</h2>
<p>å‚è€ƒä»¥ä¸‹ä¸¤ç¯‡è®ºæ–‡ï¼Œè®­ç»ƒçš„æ—¶å€™è¦ç”¨åˆ° ImageNet DET å’Œ VID ä¸¤ä¸ªæ•°æ®é›†ã€‚</p>
<blockquote>
<p><a href="../papers/T-CNN.pdf">T-cnn: Tubelets with convolutional neural networks for object detection from videos.</a><br/>
[Multi-Class Multi-Object Tracking using Changing Point Detection](â€¦/papers/Multi-Class_Multi-Object_Tracking_using_Changing Point_Detection.pdf)</p>
</blockquote>
<p>è®­ç»ƒåˆ†ä¸¤ä¸ªé˜¶æ®µï¼š</p>
<ol>
<li>ä½¿ç”¨ DET æ•°æ®é›†æ¥è®­ç»ƒ N~feat å’Œ N~det~ï¼ˆä½¿ç”¨çš„æ ‡æ³¨æ•°æ®æ˜¯ VID ä¸­çš„ 30 ä¸ªåˆ†ç±»ï¼‰ï¼Œç›¸å…³ç»†èŠ‚ï¼š</li>
</ol>
<pre><code>+ ä½¿ç”¨äº† SGD (one image at each mini-batch)
+ ä½¿ç”¨ 4 ä¸ª GPU æ¥è·‘ 120K æ¬¡è¿­ä»£ (each GPU holding one mini-batch)
+ The learning rates are 10^âˆ’3^ and 10^âˆ’4^ in the first 80K and in the last 40K iterations
</code></pre>
<ol start="2">
<li>ä½¿ç”¨ VID æ•°æ®é›†æ¥è®­ç»ƒæ•´ä¸ª FGFA æ¨¡å‹ï¼Œç›¸å…³ç»†èŠ‚ï¼š</li>
</ol>
<pre><code>+ ä½¿ç”¨ 4 ä¸ª GPU æ¥è·‘ 60K æ¬¡è¿­ä»£
+ The learning rates are 10^âˆ’3^ and 10^âˆ’4^ in the first 40K and in the last 20K iterations
</code></pre>
<p>åœ¨è®­ç»ƒå’Œæµ‹è¯•çš„æ—¶å€™ï¼Œå›¾åƒä¼šè¿›è¡Œç¼©æ”¾ï¼š</p>
<ul>
<li>åœ¨ N~feat~ ä¸­ï¼Œç¼©æ”¾æˆçŸ­è¾¹ä¸º 600px</li>
<li>åœ¨ N~flow~ ä¸­ï¼Œç¼©æ”¾æˆçŸ­è¾¹ä¸º 300px</li>
</ul>
<h2>5. Resources</h2>
<ul>
<li><a href="../papers/FGFA.pdf">Flow-Guided Feature Aggregation for Video Object Detection</a></li>
<li><a href="https://github.com/msracver/Flow-Guided-Feature-Aggregation">GitHub (python)</a></li>
</ul>
</body>
</html>