<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Towards High Performance Video Object Detection for Mobiles 学习笔记</title>
<link rel="shortcut icon" href="../../assets/img/icon.png" type="image/x-icon"/>
<link rel="icon" href="../../assets/img/icon.png" type="image/x-icon"/>
<link rel="stylesheet" href="../../assets/css/github-markdown.css">
<link rel="stylesheet" href="../../assets/css/index.css">
</head>
<body class="markdown-body">
<p><a href="https://painterdrown.github.io">painterdrown Blog</a> - <a href="https://painterdrown.github.io/cv">painterdrown CV</a></p>
<h1 id="towards-high-performance-video-object-detection-for-mobiles-学习笔记"><a class="markdownIt-Anchor" href="#towards-high-performance-video-object-detection-for-mobiles-学习笔记">#</a> Towards High Performance Video Object Detection for Mobiles 学习笔记</h1>
<blockquote>
<p>⏰ 2018-06-03 13:10:07<br/>
👨🏻‍💻 painterdrown</p>
</blockquote>
<p><ul class="markdownIt-TOC">
<li><a href="#towards-high-performance-video-object-detection-for-mobiles-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0">Towards High Performance Video Object Detection for Mobiles 学习笔记</a>
<ul>
<li><a href="#0-abstract">0. Abstract</a></li>
<li><a href="#1-introduction">1. Introduction</a></li>
<li><a href="#2-revisiting-video-object-detection-baseline">2. Revisiting Video Object Detection Baseline</a></li>
<li><a href="#3-model-architecture-for-mobiles">3. Model Architecture for Mobiles</a>
<ul>
<li><a href="#31-light-flow">3.1. Light Flow</a></li>
<li><a href="#32-flow-guided-gru-based-feature-aggregation">3.2. Flow-guided GRU based Feature Aggregation</a></li>
<li><a href="#33-lightweight-key-frame-object-detector">3.3. Lightweight Key-frame Object Detector</a></li>
<li><a href="#34-end-to-end-training">3.4. End-to-end Training</a></li>
</ul>
</li>
<li><a href="#4-resources">4. Resources</a></li>
</ul>
</li>
</ul>
</p>
<h2 id="0-abstract"><a class="markdownIt-Anchor" href="#0-abstract">#</a> 0. Abstract</h2>
<blockquote>
<p><a href="https://painterdrown.github.io/cv/thp">Towards High Performance Video Object Detection 学习笔记</a></p>
</blockquote>
<p>尽管已经在视频目标检测取得了不错的效果（参考上面 👆 的学习笔记），但是目前的架构对于手机来说还是太重了，也不清楚将 sparse feature propagation 以及 multi-frame feature aggregation 应用在计算资源比较有限的手机里，效果会受多大的影响。——因此，这篇论文提出了一个轻量级（聚焦于 sparse key frames）的视频目标检测框架。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction">#</a> 1. Introduction</h2>
<p>要把目前最好的这套视频目标检测框架移植到手机设备上，还是存在着一些问题。比如说，特征传播和特征聚集共同的 flow estimation 离要在手机上做到实时，还有比较大的一段差距。做聚集的时候，由于手机内存的限制，也不能取太大的帧区间。</p>
<p>要在手机上做视频目标检测，还是要秉持两大原则：</p>
<ol>
<li>对关键帧做特征聚集。</li>
<li>对非关键帧做特征传播。</li>
</ol>
<p>当时这两者的结构都需要针对手机进行重新设计：</p>
<ol>
<li>使用一个轻量级的深度神经网络 <strong>Light Flow</strong> 来做 flow feature estimation（手机能够无压承受）。</li>
<li>对于稀疏关键帧，用 <strong>flow-guided GRU</strong> (<a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">Gated Recurrent Unit</a>，后面的 Resources 里有视频链接) 来做特征聚集。</li>
<li>改用轻量的目标检测网络，参考：</li>
</ol>
<ul>
<li><a href="https://www.di.ens.fr/data/publications/papers/phd_sifre.pd">Rigid-motion scattering for image classification</a>（直接给链接了，速度太慢我没下载）</li>
<li><a href="../papers/Light-Head_R-CNN.pdf">Light-Head R-CNN</a></li>
</ul>
<h2 id="2-revisiting-video-object-detection-baseline"><a class="markdownIt-Anchor" href="#2-revisiting-video-object-detection-baseline">#</a> 2. Revisiting Video Object Detection Baseline</h2>
<p>关于 <strong>Sparse Feature Propagation</strong> 和 <strong>Multi-frame Feature Aggregation</strong> 我就不再赘述了。</p>
<p>这两个准则在桌面 GPU 下的表现很好。手机上的准则要做一些调整：</p>
<ul>
<li>只对关键帧做特征提取和聚集，对非关键帧做轻量的特征传播。</li>
<li>用 FlowNet 在手机上做 flow estimation 比较奢侈，FLOPS (floating point operations per second) 是 MobileNets 的 11.8 倍（在相同的输入分辨率下）。因此急需一个更轻量的 N<sub>flow</sub>。</li>
<li>应该在对齐（根据流信息）的特征图上做聚集，否则，大目标的位移会给聚集带来误差。要想办法做长区间的特征聚集（就要克服内存不足的问题）。</li>
<li>用于计算关键帧特征的卷积网络要尽量小。</li>
</ul>
<h2 id="3-model-architecture-for-mobiles"><a class="markdownIt-Anchor" href="#3-model-architecture-for-mobiles">#</a> 3. Model Architecture for Mobiles</h2>
<p><img src="images/architecture.png" alt=""></p>
<h3 id="31-light-flow"><a class="markdownIt-Anchor" href="#31-light-flow">#</a> 3.1. Light Flow</h3>
<p>FlowNet 是 pixel-level 的光流估算网络。FlowNet 先通过卷积对输入图像进行编码得到特征图（大小变为原来的 1/64），解码的时候，特征图经过几个反卷积层，得到高分辨率的流精度信息。在每个反卷积层的后面，结果特征图都会与解码器的最后一个特征图联系起来，两者共享空间分辨率以及一个未被采样的较为粗糙的流精度。解码器中的特征图会各自经过 optical flow predictors，然后根据损失函数选取精度最好的 predictor 用于 inference。</p>
<p>基于 FlowNet，<strong>Light Flow</strong> 做了几处修改，稍微损失了精度，但是理论上能提速 65 倍。</p>
<ul>
<li>在 encoder，卷积是计算的瓶颈。受 <a href="../papers/MobileNets.pdf">MobileNets</a> 的激发，所有卷积层重新采用 3×3 的深度可分离卷积 (3×3 depthwise convolution + 1×1 pointwise convolution)。这样做可以提速 8~9 倍。</li>
<li>在 decoder，每个反卷积操作都用 nearest-neighbor upsampling（最近邻居向上采样）替代，这可以通过标准卷积来达到反卷积 checkerboard artifacts（这里不了解）的作用。接着，再在标准卷积上做深度可分离的分解以减少计算。</li>
</ul>
<p>此外，与 FlowNet 不同的是，作者做的是 multi-resolution prediction。对 multi-resolution 进行向上采样，达到一个与最佳预测相同的空间分辨率，之后取均值作为最终的 prediction。训练的时候，也只有一个作用在 averaged optical flow prediction 上面的损失函数，这样能减少端点 10% 的错误率。</p>
<p><img src="images/light_flow.png" alt=""></p>
<p>Light Flow 的架构如上表所示，每一个卷积操作后面都跟有一个 batch normalization 以及 Leaky ReLU nonlinearity。Light Flow 是在 Flying Chairs dataset 上预训练的。</p>
<p>此外还有两个方法能加速 Light Flow：</p>
<ol>
<li>输入尺寸取 N<sub>feat</sub> 的一半，输出层的步长设为 4（N<sub>feat</sub> 的步长为 16），相当于做了下采样。</li>
<li>在检测网络中，对中间过程的特征图做稀疏特征传播，进一步减少非关键帧上的计算。</li>
</ol>
<h3 id="32-flow-guided-gru-based-feature-aggregation"><a class="markdownIt-Anchor" href="#32-flow-guided-gru-based-feature-aggregation">#</a> 3.2. Flow-guided GRU based Feature Aggregation</h3>
<p>Recursive aggregation 能够更好地结合邻近帧的信息，但是很难被训练来做长区间的特征聚集。研究表明：GRU 相比于 LSTM, RNN 更适合将长期的依赖关系进行模型化 (modeling long-term dependencies)。因此，作者将 <strong>convolutional GRU</strong> 收纳入 FGFA 中，聚集操作更改为：</p>
<p><img src="images/aggregation.png" alt=""></p>
<p>相比原生的 GRU，有下面三点区别：</p>
<ol>
<li>用的是 3×3 的卷积而不是全连接的矩阵乘法</li>
<li>激活函数用的是 ReLU 而不是 tanh，速度更快，更容易收敛</li>
<li>GRU 只应用在关键帧上</li>
</ol>
<h3 id="33-lightweight-key-frame-object-detector"><a class="markdownIt-Anchor" href="#33-lightweight-key-frame-object-detector">#</a> 3.3. Lightweight Key-frame Object Detector</h3>
<ul>
<li><strong>Feature Network</strong>: MobileNets 去掉最后一层的平均池化以及其他的全连接层，保留卷积层</li>
<li><strong>Detection Network</strong>: RPN + Light-Head R-CNN，RPN 中间的特征图缩小一半的通道。最后是 RoI 的 warped feature 输入到两个 sibling（分类和回归）全连接层，得到最终的结果。</li>
</ul>
<h3 id="34-end-to-end-training"><a class="markdownIt-Anchor" href="#34-end-to-end-training">#</a> 3.4. End-to-end Training</h3>
<p>N<sub>feat</sub>, N<sub>det</sub> 和 N<sub>flow</sub> 都可以连接在一起，进行端到端的训练 (for video object detection task)。</p>
<h2 id="4-resources"><a class="markdownIt-Anchor" href="#4-resources">#</a> 4. Resources</h2>
<ul>
<li><a href="../papers/Towards_High_Performance_Video_Object_Detection_for_Mobiles.pdf">Towards High Performance Video Object Detection for Mobiles</a></li>
<li><a href="https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru">吴恩达讲解 GRU</a></li>
</ul>
</body>
</html>