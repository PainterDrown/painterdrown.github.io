<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Deep Feature Flow for Video Recognition 学习笔记</title>
<link rel="shortcut icon" href="../../assets/img/icon.png" type="image/x-icon"/>
<link rel="icon" href="../../assets/img/icon.png" type="image/x-icon"/>
<link rel="stylesheet" href="../../assets/css/github-markdown.css">
<link rel="stylesheet" href="../../assets/css/index.css">
</head>
<body class="markdown-body">
<p><a href="https://painterdrown.github.io">painterdrown Blog</a> - <a href="https://painterdrown.github.io/cv">painterdrown CV</a></p>
<h1 id="deep-feature-flow-for-video-recognition-学习笔记"><a class="markdownIt-Anchor" href="#deep-feature-flow-for-video-recognition-学习笔记">#</a> Deep Feature Flow for Video Recognition 学习笔记</h1>
<blockquote>
<p>⏰ 2018-06-01 00:00:00<br/>
👨🏻‍💻 painterdrown</p>
</blockquote>
<p><ul class="markdownIt-TOC">
<li><a href="#deep-feature-flow-for-video-recognition-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0">Deep Feature Flow for Video Recognition 学习笔记</a>
<ul>
<li><a href="#0-abstract">0. Abstract</a></li>
<li><a href="#1-introduction">1. Introduction</a></li>
<li><a href="#2-related-work">2. Related Work</a></li>
<li><a href="#3-deep-feature-flow">3. Deep Feature Flow</a>
<ul>
<li><a href="#31-inference">3.1. Inference</a></li>
<li><a href="#32-training">3.2. Training</a></li>
<li><a href="#33-inference-complexity-analysis">3.3. Inference Complexity Analysis</a></li>
<li><a href="#34-key-frame-scheduling">3.4. Key Frame Scheduling</a></li>
</ul>
</li>
<li><a href="#4-network-architectures">4. Network Architectures</a>
<ul>
<li><a href="#41-flow-network">4.1. Flow Network</a></li>
<li><a href="#42-feature-network">4.2. Feature Network</a></li>
<li><a href="#43-semantic-segmentation">4.3. Semantic Segmentation</a></li>
<li><a href="#44-object-detection">4.4. Object Detection</a></li>
</ul>
</li>
<li><a href="#5-future-work">5. Future Work</a></li>
<li><a href="#6-resources">6. Resources</a></li>
</ul>
</li>
</ul>
</p>
<p>Oh yeah, 终于开始看视频目标检测啦！BTW，🎈 <em>Happy Children’s Day</em> 🎈</p>
<h2 id="0-abstract"><a class="markdownIt-Anchor" href="#0-abstract">#</a> 0. Abstract</h2>
<p>目前的图像目标检测已经发展地比较成熟了，不过如果单帧地应用在视频上则显得太慢了。作者提出了 deep feature flow（深度特征流，一个又快又准的视频目标检测框架。它只在 sparse key frames 上跑卷积，通过 deep feature maps 在同一个流场（我理解为一段连续的且比较相似的帧）的其它帧之间进行传播。训练是端到端的，速度和精度（精度有吗？感觉还不如 Faster R-CNN）上都有明显的提升。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction">#</a> 1. Introduction</h2>
<p>从图像识别到视频识别，要处理好视频帧之间的连续性以及冗余的信息。现在大多数的 CNN 架构里面，卷积计算占计算资源消耗的主体。计算出来的 convolutional feature maps 保留了图像的低级内容到中高级语义内容的对应关系。这样一来，就可以轻易地在相邻帧之间传播特征信息（by spatial warping 空间变形），这与光流类似。</p>
<p>一般来说，flow estimation &amp; feature propagation 比简单地去计算多帧的 convolutional features 要快得多。而且如果能用网络做 flow estimation 的话，整个视频目标检测架构就是端到端的。</p>
<p><img src="images/motivation.png" alt=""></p>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work">#</a> 2. Related Work</h2>
<ul>
<li>
<p><strong>Image Recognition</strong>。已经有很多成熟的网络了，可以看我前面图像目标识别的学习笔记，这里不赘述。</p>
</li>
<li>
<p><strong>Network Acceleration</strong>。比如 Fast R-CNN 里面的矩阵分解；比如 <a href="../papers/Quantized_Neural_Networks.pdf">Quantized Neural Networks</a> 的参数量子化（暂未了解）。</p>
</li>
<li>
<p><strong>Optical Flow（光流）</strong>。这个我之前在研究生学习班里几次听到这个概念。它是视频分析的基本任务，目前主要关注的是 small displacements，最近逐渐转向 large displacements，以及加入 combinatorial matching（组合匹配）这个概念。现在的深度学习已经逐渐漫延到光流这一块，<a href="../papers/FlowNet.pdf">FlowNet</a> 用深层 CNN 来估算视频中的 motion 并且有不错的效果。总而言之，光流在计算机视觉方面有奇效，比如 pose estimation（姿态估计），应该探索如何应用于视频目标检测中。</p>
</li>
<li>
<p><strong>Exploiting Temporal Information in Video Recognition</strong>。这里要解决的问题是，如何挖掘视频中的时间信息？在 <a href="../papers/T-CNN.pdf">T-CNN</a> 里面，其融合了时间和上下文信息；<a href="../papers/FSO.pdf">FSO</a> 提出了大范围的时空正则化；<a href="../papers/STFCN.pdf">STFCN</a> 用一个时空的 FCN 来做视频语义分割。虽然这些方法提升了精度，但是还是需要大量的计算，影响了速度。</p>
</li>
<li>
<p><strong>Slow Feature Analysis</strong>。在视频里面，高级的语义概念往往比低级的图像外观演化得更慢，因此这些深度的特征在视频帧之间的变化十分平滑。</p>
</li>
<li>
<p><strong><a href="../papers/Clockwork_Convnets.pdf">Clockwork Convnets</a></strong>。它做的事情跟 Deep Feature Flow 很类似：都是使网络中的某些层在视频中的某些帧上失效以及特征复用。但是由于没有考虑到不同帧之间的对应信息，Clockwork Convnets 在加速的时候会损失较多的精度。</p>
</li>
</ul>
<h2 id="3-deep-feature-flow"><a class="markdownIt-Anchor" href="#3-deep-feature-flow">#</a> 3. Deep Feature Flow</h2>
<p><img src="images/architecture.png" alt=""></p>
<h3 id="31-inference"><a class="markdownIt-Anchor" href="#31-inference">#</a> 3.1. Inference</h3>
<p>总网络（feed-forward convolutional neutral）可以分解为连续的两部分：<strong>N<sub>feat</sub>（特征网络）</strong> 和 <strong>N<sub>task</sub>（任务网络）</strong>。前者（全卷积）会输出许多 feature maps，后者会在这些 feature maps 上面做检测任务。</p>
<p>通过流估计算法得到 flow field（我理解为关键帧及其相邻帧之间的流信息）。通过双线性插值来做 feature warping。为了减少流估计算法带来的误差，通过 “scale field” 来做更好的特征估计。</p>
<p>伪代码如下：</p>
<p><img src="images/code.png" alt=""></p>
<h3 id="32-training"><a class="markdownIt-Anchor" href="#32-training">#</a> 3.2. Training</h3>
<p>Flow function 本来是用来获取低级的图像信息的，不适用于高级 feature maps 的提取。作者用一个 CNN 去估算 flow fiel 和 scale field。因此，就可以进行端到端的训练。</p>
<p>训练的方法是 <strong>SGD（随机梯度下降）</strong>。在每一个 mini-batch 里面，随机地到连续的 10 帧里面随机抽取两帧 I<sub>k</sub>（关键帧） 和 I<sub>i</sub>。前面说到的 <strong>N<sub>feat</sub></strong> 跑在 I<sub>k</sub> 上，然后 <strong>flow network N<sub>flow</sub></strong> 跑在 I<sub>k</sub> 和 I<sub>i</sub> 上来算出 flow field 和 scale field。N<sub>flow</sub> 的速度要比 N<sub>feat</sub> 快很多，前者是在 <em>Flying Chairs dataset</em> 上预训练的。在 N<sub>flow</sub> 最后输出层加上一个 sibling：scale function。</p>
<p>值得一提的是，视频检测的训练集标注的代价很高（特别是对于采用 per-frame 做法的网络）。但是 DFF 则不同，它只要求 I<sub>i</sub> 帧的数据集有标准就足够了。</p>
<h3 id="33-inference-complexity-analysis"><a class="markdownIt-Anchor" href="#33-inference-complexity-analysis">#</a> 3.3. Inference Complexity Analysis</h3>
<p>这一段主要是在分析 inference 的复杂度，inference 可以理解为用训练出来的模型去 test。分析的过程参考原论文吧。</p>
<h3 id="34-key-frame-scheduling"><a class="markdownIt-Anchor" href="#34-key-frame-scheduling">#</a> 3.4. Key Frame Scheduling</h3>
<p>这篇论文用的方法是固定长度地选取一个关键帧。但是作者提到，最理想的做法是将发生剧烈变化的帧作为关键帧。</p>
<h2 id="4-network-architectures"><a class="markdownIt-Anchor" href="#4-network-architectures">#</a> 4. Network Architectures</h2>
<h3 id="41-flow-network"><a class="markdownIt-Anchor" href="#41-flow-network">#</a> 4.1. Flow Network</h3>
<p>这里用的是 <a href="../papers/FlowNet.pdf">FlowNet</a> 的简单版本：将每一层卷积核的数目减少一半，这样整个网络的复杂度就变为原来的四分之一。再者，加入 <em>FlowNet Inception</em> 并且将其复杂度降低为 FlowNet 的八分之一。</p>
<h3 id="42-feature-network"><a class="markdownIt-Anchor" href="#42-feature-network">#</a> 4.2. Feature Network</h3>
<p>这里用的是在 ImageNet 上预训练的 <strong>ResNet-50</strong> 和 <strong>ResNet-101</strong>（取消了最后的 1000-way 分类层）。为了生成更稠密的 feature maps，将其步长从 32 减小到 16。后面：</p>
<ul>
<li>如果做语义切割，则用 <strong>DeepLab</strong>；</li>
<li>如果做目标检测，则用 <strong>R-FCN</strong>。</li>
</ul>
<h3 id="43-semantic-segmentation"><a class="markdownIt-Anchor" href="#43-semantic-segmentation">#</a> 4.3. Semantic Segmentation</h3>
<p>暂时对这里没兴趣。</p>
<h3 id="44-object-detection"><a class="markdownIt-Anchor" href="#44-object-detection">#</a> 4.4. Object Detection</h3>
<blockquote>
<p>参考 <a href="https://painterdrown.github.io/cv/rfcn">R-FCN 学习笔记</a>，这里不做赘述。</p>
</blockquote>
<h2 id="5-future-work"><a class="markdownIt-Anchor" href="#5-future-work">#</a> 5. Future Work</h2>
<p>由于要研究视频目标检测这一块，所以有必要了解一下这个话题。</p>
<blockquote>
<ol>
<li>How the joint learning affects the flow quality?</li>
</ol>
</blockquote>
<p>作者目前无法对此进行估算，因为缺少 ground truth（真实数据）。目前传统的光流技术受限于人造的或者小的数据集，对于深度学习来说数据还是很稀缺的。</p>
<blockquote>
<ol start="2">
<li>How to benefit from improvements in flow estimation and key frame scheduling?</li>
</ol>
</blockquote>
<p>关键帧的选择会较大地影响后面的检测效果。作者前面提到，这篇论文是定长地选取关键帧。然而最佳的做法是选取变化最大的帧作为关键帧，目前来说这可能会消耗过多的计算资源，希望在将来能在将这个 task 放在网络中来做。</p>
<h2 id="6-resources"><a class="markdownIt-Anchor" href="#6-resources">#</a> 6. Resources</h2>
<ul>
<li><a href="../papers/DFF.pdf">Deep Feature Flow for Video Recognition</a></li>
<li><a href="https://github.com/msracver/Deep-Feature-Flow">GitHub (python)</a></li>
</ul>
</body>
</html>